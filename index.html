<!DOCTYPE html>
<html lang="en">
<head>



  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-130924679-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-130924679-1');
  </script>


  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Ilias Zadik</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/Yale.png">

</head>
<body>
  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">

    <div class="row" style="margin-top: 5%">
      <div class="seven columns">
        <h1>Ilias Zadik</h1>
        <h5><!--Ph.D. Candidate <br> -->
          <a class="hidelink" href="https://yale.edu/">Yale University</a>, <br>
          <a class="hidelink" href="https://statistics.yale.edu/">Department of Statistics and Data Science.</a><br>

        </h5>
        <p>
          <b>Contact</b>:
           ilias.zadik at yale.edu
          <br> <b>Links to</b>:
          <a class="hidelink" href="files/Ilias_Zadik_Resume.pdf">CV</a> (last update 10/23/22), <a class="hidelink" href="https://scholar.google.co.il/citations?user=okLwXk0AAAAJ&hl=en">Google Scholar</a>, <a class="hidelink" href="https://arxiv.org/search/?searchtype=author&query=Zadik%2C+I">arXiv</a>.
        </p>
      </div>
      <div class="five columns">
        <img src="images/Yale40.jpg" alt="headshot" width="100%">
      </div>
    </div>

    <div class="row">
      <div class="twelve columns" style="margin-top: 0%">
        <h5>About me</h5>
        <p>

I am an Assistant Professor at <a class="hidelink" href="https://yale.edu/">Yale University,</a> in the <a class="hidelink" href="https://statistics.yale.edu/">Department of Statistics and Data Science.</a>
<br>
<br>
I am searching for highly motivated <strong>PhD students.</strong>

If you are interested to work with me, apply to our <a class="hidelink" href="https://statistics.yale.edu/admissions">PhD program</a> and mention my name.

<br>

<br>
      <strong> Currently teaching (Spring 2024): </strong> S&DS 351 01 Stochastic Processes  <a class="hidelink" href="https://yale.instructure.com/courses/95080">Canvas Link</a><br>
<br>

<br>

        <strong>Research Interests </strong>   <br>
  My research lies broadly in the interface of high dimensional statistics, the theory of machine learning, the theory of computation, and probability theory. A lot of my work has the goal to build and use mathematical tools to bring insights into the computational and statistical challenges of modern machine learning tasks.
  Four directions that I have been recently focusing on are:
  <ul>

    <li> Computational-statistical trade-offs in inference (see papers 4, 12, 16, 20, 22, 24, 27, 30 below).

    </li>



    <li> Phase transition phenomena (e.g. the "All-or-Nothing phenomenon") (see papers 9, 11, 13, 17, 19, 27, 28, 29 below).

    </li>

    <li> The power of lattice-based methods in inference (see papers 7, 18, 20 below).

    </li>

    <li> The cost of (differential) privacy in statistics (see papers 6, 14, 21  below).

    </li>




    </ul>


    <br>

            <br>     <strong>Short Bio (prior to current position) </strong>
  <br>
         From September 2021 to August 2023 I was a postdoctoral researcher at <a class="hidelink" href="http://mit.edu/">Massachussets Institute of Technology (MIT)</a>, <a class="hidelink" href="https://math.mit.edu/">Mathematics Department,</a> where I worked under the wonderful mentorship of <a class="hidelink" href="https://math.mit.edu/~elmos/">Elchanan Mossel</a> and <a class="hidelink" href="https://math.mit.edu/~nsun/">Nike Sun</a>, as a member of the NSF/Simons program <a class="hidelink" href="https://deepfoundations.ai/">Collaboration on the Theoretical Foundations of Deep Learning</a>.
<br>
Prior to this, from September 2019 to August 2021 I was a postdoctoral fellow at the <a class="hidelink" href="https://cds.nyu.edu/">Center for Data Science</a> of <a class="hidelink" href="https://nyu.edu/">New York University</a> and a member of it's <a class="hidelink" href="https://mad.cds.nyu.edu/">Math and Data (MaD)</a> group.
<br>
          I received my PhD on September 2019 from the <a class="hidelink" href="http://orc.mit.edu/">Operations Research Center</a> of <a class="hidelink" href="http://mit.edu/">MIT </a>, where I was very fortunate to be advised by Prof. <a class="hidelink" href="http://www.mit.edu/~gamarnik/home.html">David Gamarnik</a>. A copy of my PhD thesis can be found <a class="hidelink" href="files/PhDThesisZadikFinal.pdf">here.</a>
  <br>
          From June 2017 to August 2017 I was an intern at the <a class="hidelink" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/">Microsoft Research Lab in New England</a>, mentored by  <a class="hidelink" href="https://www.microsoft.com/en-us/research/people/jchayes/"> Jennifer Chayes </a> and <a class="hidelink" href="https://www.microsoft.com/en-us/research/people/borgs/">Christian Borgs </a>. Prior joining MIT, I completed a Master of Advanced Studies in Mathematics (Part III of the Mathematical Tripos) at the <a class="hidelink" href="https://www.maths.cam.ac.uk/postgrad/mathiii">University of Cambridge</a> and a BA in Mathematics from the Mathematics Department at the <a class="hidelink" href="http://noether.math.uoa.gr/">University of Athens</a>.



<hr>
      <h5> Some recent recorded talks </h5>
  <ul>


    <li> (1hour) <a class="hidelink" href="http://www.birs.ca/events/2022/5-day-workshops/22w5185/videos/watch/202211171630-Zadik.html">"Revisiting Jerrum’s Metropolis Process for the Planted Clique Problem."</a>
          <br>  BIRS workshop on "Learning in Networks: Performance Limits and Algorithms", November 2022



    </li>


    <li> (40mins) <a class="hidelink" href="https://www.youtube.com/watch?v=pp7F2WpV6IQ&ab_channel=SimonsInstitute">"On the Second Kahn-Kalai conjecture"</a>
          <br>  Simons workshop on Graph Limits: Nonparametric Models, and Estimation, September 2022.



    </li>



    <li> (1 hour) <a class="hidelink" href="https://www.youtube.com/watch?v=M33_3VR2WXQ&ab_channel=SimonsInstitute">"On the Cryptographic Hardness of Learning Single Periodic Neurons."</a>
          <br>  Simons workshop on "Average-Case Complexity: From Cryptography to Statistical Learning", November 2021.



    </li>


    <li> (40mins) <a class="hidelink" href="https://www.youtube.com/watch?v=QGUEaXII4Kw&ab_channel=SimonsInstitute">"The Surprising Power of the Lenstra-Lenstra-Lovasz Algorithm for Noiseless Inference."</a>
          <br>  Simons joint IFML/CCSI Symposium, November 2021.



    </li>

    <li> (25mins) <a class="hidelink" href="https://www.youtube.com/watch?v=aEx5iRi2E60&ab_channel=SimonsInstitute">"The Overlap Gap Property For Inference: Power And Limitations."</a>
          <br>  Simons workshop on "Rigorous Evidence for Information-Computation Trade-offs", September 2021.



    </li>

    <li> (25mins) <a class="hidelink" href="http://www.birs.ca/events/2021/5-day-workshops/21w5108/videos/watch/202108111120-Zadik.html">"The All-or-Nothing Phenomenon: the case of Sparse Tensor PCA."</a>
          <br>  BIRS workshop on "Random Graphs and Statistical Inference: New Methods and Applications", August 2021



    </li>


    <li> (45mins) <a class="hidelink" href="files/NYCAC2021.mp4">"MCMC Lower Bounds for Sparse PCA."</a>
          <br>  New York Colloquium on Algorithms and Complexity (NYCAC), March 2021.



    </li>


    <li> (15mins) <a class="hidelink" href="https://www.youtube.com/watch?v=EUB97Vg3Moo&list=PLXbm_p69BsBI3WYZoHuuzQaKNfPnxnB2s&index=20&ab_channel=ColumbiaUniversity%2CProbability">"The Landscape of the Planted Clique Model".</a>
          <br>  19th Northeast Probability Seminar, November 2020.



    </li>

    <li> (15mins) <a class="hidelink" href="files/Informs2020Zadik.mp4">"Computational Barriers in High Dimensional Inference. Insights from Statistical Physics."</a>
          <br>  INFORMS Annual Meeting, November 2020.








    </ul>

<hr>



      <h5> Research papers (published or submitted)</h5>  <i>(Note: the order of the authors is alphabetical, unless denoted by (*))</i>


      <br>


<ol reversed>


  <h5>2024+ </h5>

  <li>
    <a class="hidelink" href="https://arxiv.org/abs/2403.17766"><strong>Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph</strong></a>
      <br>
        <i>ArXiv preprint</i>
      <br> Xifan Yu, Ilias Zadik, Peiyuan Zhang.
  </li>


  <li>
    <a class="hidelink" href="https://arxiv.org/abs/2403.11963"><strong>Transfer Learning Beyond Bounded Density Ratios</strong></a>
      <br>
        <i>ArXiv preprint</i>
      <br> Alkis Kalavasis, Ilias Zadik, Manolis Zampetakis.
  </li>


  <li>
    <a class="hidelink" href="https://arxiv.org/abs/2311.04204"><strong>Sharp Thresholds Imply Circuit Lower Bounds: from random 2-SAT to Planted Clique</strong></a>
      <br>
        <i>ArXiv preprint</i>
      <br> David Gamarnik, Elchanan Mossel, Ilias Zadik.
  </li>


  <li>
    <a class="hidelink" href="https://arxiv.org/abs/2209.11347"><strong>A Second Moment Proof of the Spread Lemma</strong></a>
      <br>
        <i>ArXiv preprint</i>
      <br> Elchanan Mossel, Jonathan Niles-Weed, Nike Sun, Ilias Zadik.
  </li>

  <li>
    <a class="hidelink" href="https://arxiv.org/abs/2209.03326"><strong>On the Second Kahn-Kalai Conjecture</strong></a> (<a class="hidelink" href="https://www.youtube.com/watch?v=pp7F2WpV6IQ&ab_channel=SimonsInstitute"><strong>45mins video - Simons workshop</strong></a>)
      <br>
        <i>ArXiv preprint</i>
      <br> Elchanan Mossel, Jonathan Niles-Weed, Nike Sun, Ilias Zadik.
  </li>



  <li>
        <a class="hidelink" href="https://arxiv.org/abs/1904.07174"><strong>The Landscape of the Planted Clique Problem: Dense Subgraphs and the Overlap Gap Property</strong></a> ( <a class="hidelink" href="https://www.youtube.com/watch?v=ZhC7wIx9qwM&ab_channel=CourantInstituteSeminars"><strong>1hr video - NYU Probability Seminar</strong></a>)
        <br>
        <i>Annals of Applied Probability, 2024+</i>
         <br> David Gamarnik, Ilias Zadik.

  </li>



  <li>
    <a class="hidelink" href="https://arxiv.org/abs/1912.01599"><strong>Stationary Points of Shallow Neural Networks with Quadratic Activation Function</strong></a> (<a class="hidelink" href="https://www.youtube.com/watch?v=iQAur0NweGw&ab_channel=MachineLearningatMIT"><strong>30mins video by Eren - MIT MLTea</strong></a>)
      <br>
        <i>Mathematics of Operations Research, 2024</i>
      <br> David Gamarnik, Eren C. Kızıldağ, Ilias Zadik.
  </li>
<h5>2023 </h5>








<li>
  <a class="hidelink" href="https://arxiv.org/abs/2302.14830"><strong>Sharp thresholds in inference of planted subgraphs</strong></a>
    <br>
      <i>Proceedings of the Conference on Learning Theory (COLT), 2023</i>
    <br> Elchanan Mossel, Jonathan Niles-Weed, Youngtak Sohn, Nike Sun, Ilias Zadik.
</li>




<li>
  <a class="hidelink" href="https://arxiv.org/abs/2204.01911"><strong>Almost-Linear Planted Cliques Elude the Metropolis Process</strong></a> (<a class="hidelink" href="http://www.birs.ca/events/2022/5-day-workshops/22w5185/videos/watch/202211171630-Zadik.html"><strong>1h video - BIRS workshop</strong></a>)
    <br>
      <i>Proceedings of the Symposium on Discrete Algorithms (SODA), 2023</i>
    <br> Zongchen Chen, Elchanan Mossel, Ilias Zadik.
</li>




<li>
  <a class="hidelink" href="https://arxiv.org/abs/2103.03379"><strong>Shapes and recession cones in mixed-integer convex representability</strong></a>
    <br>
      <i>Mathematical Programming, 2023</i>
    <br> Ilias Zadik, Miles Lubin, Juan Pablo Vielma. (*)
</li>




<h5>2022 </h5>





<li>
  <a class="hidelink" href="https://arxiv.org/abs/2205.09727"><strong>The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics</strong></a>  (<a class="hidelink" href="https://nips.cc/virtual/2022/poster/52907"><strong>5mins video - NeurIPS</strong></a>)
    <br>
      <i>Advances in Neural Information Processing Systems, (NeurIPS), 2022</i>
        <br> <strong> Selected for an Oral Presentation.</strong>
    <br> Afonso Bandeira, Ahmed El Alaoui, Sam Hopkins, Tselil Schramm, Alex Wein, Ilias Zadik.
</li>




<li>
  <a class="hidelink" href="https://arxiv.org/abs/2208.07438"><strong>Archimedes Meets Privacy: On Privately Estimating Quantiles in High Dimensions Under Minimal Assumptions</strong></a> <a class="hidelink" href="https://nips.cc/virtual/2022/poster/53219">
      <br>
      (<strong>5mins video  by Dan - NeurIPS,</strong></a>)
    <br>
      <i>Advances in Neural Information Processing Systems, (NeurIPS), 2022</i>
    <br> Omri Ben-Eliezer, Dan Mikulincer, Ilias Zadik.
</li>

<li>
  <a class="hidelink" href="https://arxiv.org/abs/2112.03898"><strong>Lattice-Based Methods Surpass Sum-of-Squares in Clustering</strong></a></strong></a> (<a class="hidelink" href="https://slideslive.com/38985603/latticebased-methods-surpass-sumofsquares-in-clustering?ref=folder-104261"><strong>20mins video by Min Jae - COLT</strong></a>)
    <br>
      <i>Proceedings of the Conference on Learning Theory (COLT), 2022</i>
    <br> Ilias Zadik, Min Jae Song, Alex Wein, Joan Bruna. (*)
</li>

<li>
  <a class="hidelink" href="http://arxiv.org/abs/2206.07640"><strong>Statistical and Computational Phase Transitions in Group Testing</strong></a></strong></a> (<a class="hidelink" href="https://slideslive.com/38985700/statistical-and-computational-phase-transitions-in-group-testing?ref=folder-104261"><strong>20mins video - COLT</strong></a>)
    <br>
      <i>Proceedings of the Conference on Learning Theory (COLT), 2022</i>
    <br> Amin Coja-Oghlan, Oliver Gebhard, Max Hahn-Klimroth, Alex Wein, Ilias Zadik.
</li>






<h5>2021 </h5>






  <li>
    <a class="hidelink" href="https://arxiv.org/abs/2106.10744"><strong>On the Cryptographic Hardness of Learning Single Periodic Neurons</strong></a></strong></a> (<a class="hidelink" href="https://www.youtube.com/watch?v=gcjCDqt9ewg&ab_channel=SimonsInstitute"><strong>20 mins video - Deep Learning Theory Symposium, Simons</strong></a>)
      <br>
        <i>Advances in Neural Information Processing Systems, (NeurIPS), 2021</i>
      <br> Min Jae Song, Ilias Zadik, Joan Bruna. (*)
  </li>

      <li>
        <a class="hidelink" href="https://arxiv.org/abs/2102.12422"><strong>It was “all” for “nothing”: sharp phase transitions for noiseless discrete channels</strong></a> (<a class="hidelink" href="http://www.learningtheory.org/colt2021/virtual/poster_1363.html"><strong>18mins video - COLT</strong></a>)
            <br>
            <i>IEEE Transactions of Information Theory, 2023</i>
            <br>
            <a class="hidelink" href="http://proceedings.mlr.press/v134/nilesweed21a/nilesweed21a.pdf"><strong> Conference version</strong></a><i> in Proceedings of the Conference on Learning Theory (COLT), 2021</i>
          <br> Jonathan Niles-Weed, Ilias Zadik.
      </li>

      <li>
        <a class="hidelink" href="https://arxiv.org/abs/2011.05258"><strong>Group testing and local search: is there a computational-statistical gap?</strong></a> (<a class="hidelink" href="https://www.ias.edu/video/computational-statistical-gaps-and-group-testing-problem"><strong>2hrs video by Fotis - IAS</strong></a>)
            <br>
            <i>Proceedings of the Conference on Learning Theory (COLT), 2021</i>
          <br> Fotis Iliopoulos, Ilias Zadik.
      </li>



      <li>
        <a class="hidelink" href="https://arxiv.org/abs/2103.01887"><strong>Self-Regularity of Non-Negative Output Weights for Overparameterized Two-Layer Neural Networks</strong></a>
            <br>
            <i>IEEE Transactions of Signal Processing, 2021</i>
            <br>
            <a class="hidelink" href="https://ieeexplore.ieee.org/abstract/document/9517811"><strong> Conference version</strong></a><i> in Proceedings of the International Symposium on Information Theory (ISIT), 2021</i>
          <br> David Gamarnik, Eren C. Kızıldağ, Ilias Zadik.
      </li>



<h5>2020 </h5>


    <li>
      <a class="hidelink" href="https://arxiv.org/abs/2011.06202"><strong>Optimal Private Median Estimation under Minimal Distributional Assumptions</strong></a> (<a class="hidelink" href="https://www.youtube.com/watch?v=RSa1lGJSHfg&ab_channel=%CE%95%CE%BC%CE%BC%CE%B1%CE%BD%CE%BF%CF%8D%CE%BB%CE%92%CE%BB%CE%B1%CF%84%CE%AC%CE%BA%CE%B7%CF%82"><strong>10mins video by Manolis - NeurIPS spotlight</strong></a>)
      <br>
      <i> Advances in Neural Information Processing Systems, (NeurIPS), 2020 </i>
      <br> <strong> Selected for a Spotlight Presentation (~5% of submitted papers).</strong>
      <br> Christos Tzamos, Manolis Vlatakis, Ilias Zadik.


    </li>


  <li>
    <a class="hidelink" href="https://arxiv.org/abs/2007.11138"><strong>The All-or-Nothing Phenomenon in Sparse Tensor PCA</strong></a> (<a class="hidelink" href="files/NeuripsPosterAll20.pdf"><strong>Poster</strong></a>, ( <a class="hidelink" href="http://www.birs.ca/events/2021/5-day-workshops/21w5108/videos/watch/202108111120-Zadik.html"><strong>25mins video - BIRS workshop</strong></a>)
    <br>
    <i> Advances in Neural Information Processing Systems, (NeurIPS), 2020</i>
    <br> Jonathan Niles-Weed, Ilias Zadik.


  </li>

  <li>
         <a class="hidelink" href="https://arxiv.org/abs/2006.10689"><strong>Free Energy Wells and the Overlap Gap Property in Sparse PCA</strong></a> ( <a class="hidelink" href="https://simons.berkeley.edu/talks/free-energy-wells-and-overlap-gap-property-sparse-pca"><strong>25mins video - Simons workshop</strong></a>)
         <br>
         <i>Communications on Pure and Applied Mathematics, 2023 </i>
         <br>
         <a class="hidelink" href="http://proceedings.mlr.press/v125/ben-arous20a/ben-arous20a.pdf"><strong> Conference version</strong></a><i> in Proceedings of the Conference of Learning Theory (COLT), 2020</i>
         <br> G&egrave;rard Ben Arous, Alex Wein, Ilias Zadik.

  </li>

<h5>2019 </h5>

        <li>
              <a class="hidelink" href="https://arxiv.org/abs/1903.05046"><strong>The All-or-Nothing Phenomenon in Sparse Linear Regression</strong></a> (<a class="hidelink" href="files/COLT19Talk.pdf"><strong>Slides</strong></a>, <a class="hidelink" href="files/COLT19Poster.pdf"><strong>Poster</strong></a>)
              <br>
              <i>Mathematical Statistics and Learning, 2021 </i>
              <br>
              <a class="hidelink" href="http://proceedings.mlr.press/v99/reeves19a/reeves19a.pdf"><strong> Conference version</strong></a> in the <i>Proceedings of the Conference on Learning Theory (COLT), 2019</i>
              <br> Galen Reeves, Jiaming Xu, Ilias Zadik.
        </li>




  <li>
  <a class="hidelink" href="https://arxiv.org/abs/1912.13027"><strong>All-or-Nothing Phenomena: From Single-Letter to High Dimensions</strong></a>
  <br>
  <i> Proceedings of the International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2019</i>
  <br> Galen Reeves, Jiaming Xu, Ilias Zadik.
</li>


<li>
      <a class="hidelink" href="http://people.lids.mit.edu/yp/homepage/data/isit2019_gaussianp.pdf"> <strong>Improved bounds on Gaussian MAC and sparse regression via Gaussian inequalities</strong></a>
       <br>
       <i>Proceedings of the International Symposium on Information Theory (ISIT), 2019</i>
       <br> Ilias Zadik, Christos Thrampoulidis, Yury Polyanskiy. (*)

</li>

<li>
       <a class="hidelink" href="https://arxiv.org/abs/1903.03949"><strong>A simple bound on the BER of the MAP decoder for massive MIMO systems</strong></a>
       <br>
       <i>Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2019</i>
       <br> Christos Thrampoulidis, Ilias Zadik, Yury Polyanskiy. (*)

</li>

<h5>2018 </h5>

      <li>
        <a class="hidelink" href="https://arxiv.org/abs/1910.10890"><strong>Inference in High-Dimensional Linear Regression via Lattice Basis Reduction and Integer Relation Detection</strong></a>
        <br>
        <i>IEEE Transactions of Information Theory, 2021</i>
        <br>
        <a class="hidelink" href="https://arxiv.org/abs/1803.06716"><strong> Conference version</strong></a> with David Gamarnik, in <i>  Advances in Neural Information Processing Systems, (NeurIPS), 2018</i>
        <br> David Gamarnik, Eren C. Kızıldağ, Ilias Zadik.
      </li>




  <li>
    <a class="hidelink" href="https://arxiv.org/abs/1810.02183"><strong>Revealing Network Structure, Confidentially: Improved Rates for Node-Private Graphon Estimation </strong> </a> (<a class="hidelink" href="files/FOCSslides.pdf"><strong>Slides</strong></a>, <a class="hidelink" href="https://www.youtube.com/watch?v=tGTXpFMlbo0"><strong>1h video by Adam -Simons</strong></a>)
    <br>
    <i>Proceedings of the Symposium on Foundations of Computer Science (FOCS), 2018</i>
    <br> Christian Borgs, Jennifer Chayes, Adam Smith, Ilias Zadik.
  </li>


        <li>
              <a class="hidelink" href="https://arxiv.org/abs/1711.00342"><strong>Orthogonal Machine Learning: Power and Limitations </strong> </a> (<a class="hidelink" href="files/ICMLslides.pdf"><strong>Slides</strong></a>, <a class="hidelink" href="files/ICMLposter.pdf"><strong>Poster</strong></a>, <a class="hidelink" href="https://github.com/IliasZadik/double_orthogonal_ml"><strong>Code</strong></a>)
              <br>
              <i>Proceedings of International Conference of Machine Learning (ICML), 2018 (20 minute Presentation)</i>
              <br> Lester Mackey, Vasilis Syrgkanis, Ilias Zadik.

            </li>
<h5>2017 </h5>


            <li>
              <a class="hidelink" href=""><strong>Sparse High-Dimensional Linear Regression. Estimating Squared Error and a Phase Transition.</strong></a>
              <br>
              <i>Annals of Statistics, 2022 </i>
              <br> David Gamarnik, Ilias Zadik.
              <br>
              This paper merges:
              <br>
              (a) <a class="hidelink" href="https://arxiv.org/abs/1701.04455"><strong>High-Dimensional Regression with Binary Coefficients. Estimating Squared Error and a Phase Transition</strong></a> (<a class="hidelink" href="files/colt_slides.pdf"><strong>Slides</strong></a>, <a class="hidelink" href="files/colt_poster.pdf"><strong>Poster</strong></a>,  <a class="hidelink" href="https://webcolleges.uva.nl/Mediasite/Play/0bc97cdb77254335b76040f763bdd39f1d?catalog=9948106f-cceb-4730-82a3-ad95faf7bc94"><strong>20mins video</strong></a>)
              <br>
              <i> Proceedings of the Conference on Learning Theory (COLT), 2017 (20 minutes Presentation) </i>
              <br>
              (b) <a class="hidelink" href="https://arxiv.org/abs/1711.04952"><strong>Sparse High-Dimensional Linear Regression. Algorithmic Barriers and a Local Search Algorithm</strong></a>
              <br>
              <i>arXiv preprint, 2017</i>

            </li>


            <li>
              <a class="hidelink" href="https://arxiv.org/abs/1706.05135"><strong>Mixed integer convex representability</strong></a> (<a class="hidelink" href="files/MICP_slides.pdf"><strong>Slides</strong></a>)
              <br>
              <i> Mathematics of Operations Research, 2021</i>
              <br>
              <a class="hidelink" href="https://arxiv.org/abs/1611.07491"><strong> Conference version</strong></a><i> in Proceedings of the International Conference of Integer Programming and Combinatorial Optimization (IPCO), 2017</i>
              <br> Miles Lubin, Juan Pablo Vielma, Ilias Zadik.

  </li>

  <h5> Pre-2017 (complex analysis): </h5>





                              <li>
                                <a class="hidelink" href="https://arxiv.org/abs/1310.1509"><strong>Universal Padé approximants and their behaviour on the boundary</strong></a>
                                <br>
                                <i>Monatshefte für Mathematik, Vol. 182, p.p. 173–193</i>, 2017
                                <br> Ilias Zadik.
                              </li>


                            <li>
                              <a class="hidelink" href="https://arxiv.org/abs/1212.4394"><strong>Pade approximants, density of rational functions in A^(infinity)(V) and smoothness of the integration operator</strong></a>
                              <br>
                              <i>Journal of Mathematical Analysis and Applications; Vol. 423, p.p. 1514–1539</i>, 2015
                              <br> Vassili Nestoridis, Ilias Zadik.
                            </li>
                            </li>

</ol>

  <h5>Thesis/Notes/Survey Articles</h5>
        <ul>
          <li>
             <a class="hidelink" href="files/PhDThesisZadikFinal.pdf"><strong>Computational and Statistical Challenges in High Dimensional Statistical Models </strong></a>
             <br>
             <i>PhD Thesis, Operations Research Center, Massachussets Institute of Technology, 2019</i>
             <br>
           </li>
        <li>
           <a class="hidelink" href="files/Note.pdf"><strong>Private Algorithms Can Always Be Extended </strong></a>
           <br>
           <i>Note on the extension of private algorithms</i>
           <br> Christian Borgs, Jennifer Chayes, Adam Smith, Ilias Zadik.
         </li>



        <li>
              <a class="hidelink" href="files/Essay.pdf"><strong>Noise Sensitivity with applications to Percolation and Social Choice Theory </strong></a>
              <br>
              <i>Part III Essay</i>, 2014
              <br> Advised by Béla Bollobás
            </li>

            <li>
               <a class="hidelink" href="https://link.springer.com/chapter/10.1007/978-1-4939-7543-3_2"><strong>A Note on the Density of Rational Functions in</strong> &Alpha;<sup>∞ </sup>(&Omega;)</a>
               <br>
               <i>A complex analysis note on the density of rational functions</i>
                 <br>
                 <i>New Trends in Approximation Theory; vol 81, p.p. 27-35</i>
               <br> Javier Falcó, Vassili Nestoridis, Ilias Zadik
             </li>

</ul>





<h5>Service</h5>

<ul>

<li> From Spring 2020 to Spring 2021 I had the pleasure to be among the organizers of the virtual <a class="hidelink" href="https://www.youtube.com/channel/UCHmajwTZztZlwyE322hpl7Q"> MaD+ seminar</a>, which run during the COVID-19 pandemic.</li>

<li>  I have served as a reviewer for Annals of Statistics, Operations Research, Probability Theory and Related Fields, Mathematical Programming, SIAM Journal of Discrete Mathematics, SIAM Journal of Optimization, Combinatorica, IEEE Journal on Selected Areas in Information Theory, and for the conferences COLT, NeurIPS, FOCS, STOC, ITCS, ISIT, ICALP and SODA.
</li>

<li>  Program Committees: <a class="hidelink" href="http://learningtheory.org/colt2023/"> COLT 2023</a>, <a class="hidelink" href="http://learningtheory.org/colt2022/"> COLT 2022</a> and <a class="hidelink" href="http://learningtheory.org/colt2021/"> COLT 2021.</a>
</li>

</ul>

<h5>Past teaching</h5>


<ul>

<i><h5.2> Instructor </h5.2></i>

<li>  Fall 2023, S&DS 688 01, (Yale): <a class="hidelink" href="https://www.notion.so/yale-topics/Computational-and-Statistical-Trade-offs-in-High-Dimensional-Statistics-1c7ab3a1178d47be9e3f963e32a2b36b">S&DS 688 01: Computational and statistical trade-offs in high dimensional statistics</a>
<br> <a>Newly designed</a> state-of-the-art reseerch level course on computational-statistical trade-offs (see link for scribe notes and details).

<li> Fall 2020, DS-GA 1005, (NYU): Inference and Representation. (Co-instructor: <a class="hidelink" href="https://cims.nyu.edu/~bruna/"> Joan Bruna</a>)
<br>   <a>Co-designed</a> and taught an advanced graduate-level course on modern theoretical aspects of statistics and machine learning.
</li>


<li>  Fall 2019, DS-GA 1002 (NYU): Probability and Statistics for Data Science. (Co-instructor: <a class="hidelink" href="https://math.nyu.edu/~cfgranda/"> Carlos Fernandez-Granda</a> )
<br>  Introductory graduate-level course on probability and statistics.
</li>




<i><h5.2> Teaching Assistant </h5.2></i>


<li>  Spring 2017,	6.265/15.070 (MIT): Modern Discrete Probability. (Instructors: <a class="hidelink" href="https://www.mit.edu/~gbresler/"> Guy Bresler</a>, <a class="hidelink" href="https://people.lids.mit.edu/yp/homepage/"> Yury Polyanskiy</a>)
<br>  Advanced graduate-level course on discrete probability and modern applications to computer science.
</li>


<li>   Fall 2016, 6.436J/15.085J (MIT): Fundamentals of Probability. (Instructor: <a class="hidelink" href="http://www.mit.edu/~gamarnik/home.html"> David Gamarnik</a> )
<br>  Introductory graduate-level course on probability.
</li>


</ul>

  </ol>

        <h5>Some awards</h5>
        <ul>

          <li>
            Honorable Mention for MIT Operations Research Center Best Student Paper Award, 2017
            <br>
            Paper (no. 4 above): <a class="hidelink" href="https://arxiv.org/abs/1701.04455"><strong>High-Dimensional Regression with Binary Coefficients. Estimating Squared Error and a Phase Transition.</strong></a>

          </li>
            <li>
              Senior Scholarship from Trinity College, University of Cambridge, 2014.
              <br> Awarded for achieving "distinction" (ranked 13th out of 247 students) in Part III examination in Mathematics.
            </li>
            <li>
              The Onassis Foundation Scholarship for Master Studies, 2013-2014
              <br> Partially supported my Master studies in the University of Cambridge.
            </li>
            <li>
              The Cambridge Home and European Scholarship Scheme (CHESS) award, 2013-2014.
              <br> Partially supported my Master studies in the University of Cambridge.
            </li>
            <li>
              International Mathematics Competition for University Students (IMC)
              <br> First Prize, 2011, Second Prize, 2010.
            </li>
            <li>
              South Eastern European Mathematics Olympiad for University Students (SEEMOUS)
              <br> Gold Medal (<strong>first place</strong>, scored 39.5/40), 2011, Silver Medal, 2010.
            </li>
        </ul>

      </div>
      </div>
      </div>





</body>
</html>
